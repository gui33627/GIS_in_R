# b) Carry out a t-test for the means of y1 for complete and missing data
t.test(y1[u > 0], y1[u < 0])
a = 2
b = 0
u = a*(y1 - 1) + b*(y2 - 5) + z3
# Compare histograms of y1 and y2 for complete cases and missing data
hist(y1[u > 0]) # MNAR
hist(y1[u < 0], add = TRUE, border = "red")
hist(y2[u > 0]) # MAR
hist(y2[u < 0], add = TRUE, border = "red")
# Carry out a t-test for the means of y1 for complete and missing data
t.test(y1[u > 0], y1[u < 0])
# Now we reject the hypothesis of equal means!
# Note also the estimates of mean, var and corr:
mean(y2[u > 0])
var(y2[u > 0])
cor(y1[u > 0], y2[u > 0])
a = 0
b = 2
u = a*(y1 - 1) + b*(y2 - 5) + z3
# Compare histograms of y1 and y2 for complete cases and missing data
hist(y1[u > 0])
hist(y1[u < 0], add = TRUE, border = "red")
hist(y2[u > 0])
hist(y2[u < 0], add = TRUE, border = "red")
# Carry out a t-test for the means of y1 for complete and missing data
t.test(y2[u > 0], y2[u < 0])
# Now we reject the hypothesis of equal means!
# Note also the estimates of mean, var and corr:
mean(y2[u > 0])
var(y2[u > 0])
cor(y1[u > 0], y2[u > 0])
# MCAR:
a = 0
b = 0
u = a*(y1 - 1) + b*(y2 - 5) + z3
# Create a new variable y with missing values, so we keep original y2:
y = y2
# Decide on the proportion of missing:
p = 0.5
# Create NAs for y's which correspond to some u's:
y[u < qnorm(p)] = NA
qnorm(p)
# Missing data indicator
Ry = as.numeric(!is.na(y))
# Complete cases
data.cc = cbind(y1,y)[Ry == 1, ]
# Removed cases due to missingness
data.dropped = cbind(y1, y)[Ry == 0, ]
# Regression of missing based on all completely observed vars:
reg = lm(y ~ y1, data = data.frame(data.cc))
# Predict the mising cases from the regression model:
y.imp = predict(reg, newdata = data.frame(data.dropped))
# Alternatively, you may use mice package command:
y.imp = mice.impute.norm.predict(y, !is.na(y), y1)
library(mice)
# Alternatively, you may use mice package command:
y.imp = mice.impute.norm.predict(y, !is.na(y), y1)
y[Ry == 0]
y.imp
# Impute the predictions where they belong:
y[Ry == 0] = y.imp
plot(y1,y)
# Calculate SSE
sum((y-y2)^2)
# Compare regression after imputation to before missingness:
summary(lm(y1~y2))
summary(lm(y1~y))
# Repeat for MAR
a = 2
b = 0
u = a*(y1 - 1) + b*(y2 - 5) + z3
# Repeat for MAR
a = 2
b = 0
u = a*(y1 - 1) + b*(y2 - 5) + z3
y = y2
y[u < 0] = NA
# Missing data indicator
Ry = as.numeric(!is.na(y))
data.cc = cbind(y1,y)[Ry == 1, ]
data.dropped = cbind(y1, y)[Ry ==0, ]
reg = lm(y ~ y1, data = data.frame(data.cc))
# Repeat for MAR
a = 2
b = 0
u = a*(y1 - 1) + b*(y2 - 5) + z3
y = y2
y[u < 0] = NA
# Missing data indicator
Ry = as.numeric(!is.na(y))
data.cc = cbind(y1,y)[Ry == 1, ]
data.dropped = cbind(y1, y)[Ry ==0, ]
reg = lm(y ~ y1, data = data.frame(data.cc))
y.imp = predict(reg, newdata = data.frame(data.dropped))
y[Ry == 0] = y.imp
# Calculate SSE
sum((y-y2)^2)
summary(lm(y1~y))
y2
# Repeat for NMAR
a = 0
b = 2
u = a*(y1 - 1) + b*(y2 - 5) + z3
y = y2
u = a*(y1 - 1) + b*(y2 - 5) + z3
y = y2
y[u < 0] = NA
# Missing data indicator
Ry = as.numeric(!is.na(y))
data.cc = cbind(y1,y)[Ry == 1, ]
data.dropped = cbind(y1, y)[Ry ==0, ]
reg = lm(y ~ y1, data = data.frame(data.cc))
y.imp = predict(reg, newdata = data.frame(data.dropped))
y[Ry == 0] = y.imp
# Calculate SSE
sum((y-y2)^2)
summary(lm(y1~y))
plot(y1, y)
cor(y,y1)
a = 0
b = 2
u = a*(y1 - 1) + b*(y2 - 5) + z3
y = y2
y[u < 0] = NA
# Missing data indicator
Ry = as.numeric(!is.na(y))
data.cc = cbind(y1,y)[Ry == 1, ]
data.dropped = cbind(y1, y)[Ry ==0, ]
reg = lm(y ~ y1, data = data.frame(data.cc))
y.imp = predict(reg, newdata = data.frame(data.dropped))
noise = rnorm(length(y.imp), 0, summary(reg)$sigma)
noise
knitr::opts_chunk$set(echo = TRUE)
# read the data with around 20% missingness in the entire data set
df_20p_NA <- read.csv("~/Documents/NYU/Courses/Spring2021/MissingData/week3/data_20p_NA.csv")
# find the numerical variables
idx_num <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "numeric" |
lapply(df_20p_NA,class) == "integer")))
df_num <- df_20p_NA[idx_num]
# find which numerical variables have missing data
apply(is.na(df_num), 2, sum)
# find which variables are fully observed variables to be used in the regression imputation
as.numeric(apply(is.na(df_20p_NA), 2, sum))
# create a new data set to store the numerical variable to be imputed and the full observed variables
df_imp <- with(df_20p_NA, data.frame( S1591 = S.1591.After.the.farmers.sell.some.of.the.crops.harvested.how.long.does.the.remaining.food.last...Months..max..12.,
M0298 = M.0298..Region, M0312 = M.0312..Returning.IDP..Individuals.,
M0360 = M.0360..Host.Community..Individual.))
str(df_imp)
sum(is.na(df_imp$S1591))/nrow(df_imp)
# regression imputation with noise
data.cc = df_imp[which(!is.na(df_imp$S1591)), ]
data.dropped = df_imp[which(is.na(df_imp$S1591)), ]
data.cc
data.dropped
class(data.cc)
reg = lm(S1591 ~ M0298 + M0312 + M0360, data = data.cc)
y.imp = predict(reg, newdata = data.dropped)
reg = lm(S1591 ~ M0312 + M0360, data = data.cc)
y.imp = predict(reg, newdata = data.dropped)
noise = rnorm(length(y.imp), 0, summary(reg)$sigma)
y.imp
noise = rnorm(length(y.imp), 0, summary(reg)$sigma)
which(!is.na(df_imp$S1591))
df_imp$S1591[which(!is.na(df_imp$S1591))]
df_imp$S1591[which(is.na(df_imp$S1591))]
y.imp + noise
y.imp + noise
df_imp$S1591[which(is.na(df_imp$S1591))] = y.imp + noise
str(df_20p_NA)
class(df_20p_NA$M.0298..Region)
class(df_20p_NA$M.0312..Returning.IDP..Individuals.)
levels(df_20p_NA$S.1587..Do.IDPs.have.access.to.land.for.cultivation.)
length( levels(df_20p_NA$S.1587..Do.IDPs.have.access.to.land.for.cultivation.))
lapply(df_20p_NA,class) == "factor"
unlist(lapply(df_20p_NA,class) == "factor")
idx_num <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "factor") ))
idx_fac <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "factor") ))
idx_fac
dichotomous <- df_20p_NA[,idx_fac]
dichotomous
lapply(dichotomous,levels)
length(lapply(dichotomous,levels))
unlist(lapply(dichotomous, function(x)  length(levels(x)) ))
which(unlist(lapply(dichotomous, function(x)  length(levels(x)) )) == 2)
idx_dich <- which(unlist(lapply(dichotomous, function(x)  length(levels(x)) )) == 2)
idx_fac <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "factor") ))
df_fac <- df_20p_NA[,idx_fac]
idx_dich <- which(unlist(lapply(df_fac, function(x)  length(levels(x)) )) == 2)
df_dich <- df_fac[,idx_dich]
str(df_dich)
idx_dich <- which(unlist(lapply(df_fac, function(x)  length(levels(x)) )) == 2)
# First read some data from the internet:
mydata <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/MissingData/week4/binary.csv")
## view the first few rows of the data
head(mydata)
mydata.miss = mydata
u = runif(nrow(mydata))
u
mydata.miss$admit[(mydata$gre < 500) & (u < 0.5)] = NA
# Notice the decision was to remove randomly 50% of admit when gre < 500
# This shows how many are now missing among admit
summary(mydata.miss)
summary(df_dich)
mydata.miss$rank <- factor(mydata$rank)
Ry = as.numeric(!is.na(mydata.miss$admit))
dat.cc = mydata.miss[Ry == 1, ]
dat.dropped = mydata.miss[Ry == 0, ]
mylogit <- glm(admit ~ gre + gpa + rank, data = dat.cc, family = "binomial")
summary(mylogit)
# We now use the model to predict the missing
y.imp <- predict(mylogit, newdata = dat.dropped, type = "response")
# Note this returns the probability so we need to convert it to binary response before imputing it:
# without noise
mydata.miss$admit[Ry == 0] = round(y.imp,0)
y.imp
?rbinom
# with noise
mydata.miss$admit[Ry == 0] = rbinom(sum(Ry==0), 1, y.imp)
?round
round(y.imp,0)
y.imp
round(y.imp,0)
names(y.imp)
values(y.imp)
value(y.imp)
# First read some data from the internet:
mydata <- read.csv("/Users/junhui/Documents/NYU/Courses/Spring2021/MissingData/week4/binary.csv")
## view the first few rows of the data
head(mydata)
mydata.miss = mydata
u = runif(nrow(mydata))
mydata.miss$admit[(mydata$gre < 500) & (u < 0.5)] = NA
# Notice the decision was to remove randomly 50% of admit when gre < 500
# This shows how many are now missing among admit
summary(mydata.miss)
mydata.miss$rank <- factor(mydata$rank)
Ry = as.numeric(!is.na(mydata.miss$admit))
dat.cc = mydata.miss[Ry == 1, ]
dat.dropped = mydata.miss[Ry == 0, ]
mylogit <- glm(admit ~ gre + gpa + rank, data = dat.cc, family = "binomial")
summary(mylogit)
# We now use the model to predict the missing
y.imp <- predict(mylogit, newdata = dat.dropped, type = "response")
y.imp
dat.dropped
y.imp
# Note this returns the probability so we need to convert it to binary response before imputing it:
# without noise
mydata.miss$admit[Ry == 0] = round(y.imp,0)
mydata.miss$admit[Ry == 0]
# with noise
mydata.miss$admit[Ry == 0] = rbinom(sum(Ry==0), 1, y.imp)
mydata.miss$admit[Ry == 0]
round(y.imp,0)
# Exercise 2: Create 50% missingness in the rank variable when GPA > 3.6
# Then practice imputation with noise for categorical with more than one level
# (pretend rank is unordered)
library(nnet)
mydata.miss = mydata
u = runif(nrow(mydata))
mydata.miss$rank[(mydata$gpa > 3.6) & (u < 0.5)] = NA
summary(mydata.miss)
mydata.miss$rank <- factor(mydata$rank)
Ry = as.numeric(!is.na(mydata.miss$rank))
dat.cc = mydata.miss[Ry == 1, ]
dat.dropped = mydata.miss[Ry == 0, ]
mmod = multinom(rank ~ gre + gpa, data = dat.cc)
ps = predict(mmod, type = "prob", newdata = dat.dropped)
ps
# Exercise 2: Create 50% missingness in the rank variable when GPA > 3.6
# Then practice imputation with noise for categorical with more than one level
# (pretend rank is unordered)
library(nnet)
mydata.miss = mydata
u = runif(nrow(mydata))
mydata.miss$rank[(mydata$gpa > 3.6) & (u < 0.5)] = NA
summary(mydata.miss)
mydata.miss$rank <- factor(mydata$rank)
Ry = as.numeric(!is.na(mydata.miss$rank))
dat.cc = mydata.miss[Ry == 1, ]
dat.dropped = mydata.miss[Ry == 0, ]
mmod = multinom(rank ~ gre + gpa, data = dat.cc)
ps = predict(mmod, type = "prob", newdata = dat.dropped)
mmod
dat.dropped
mydata.miss = mydata
u = runif(nrow(mydata))
mydata.miss$rank[(mydata$gpa > 3.6) & (u < 0.5)] = NA
summary(mydata.miss)
mydata.miss$rank <- factor(mydata$rank)
Ry = as.numeric(!is.na(mydata.miss$rank))
dat.cc = mydata.miss[Ry == 1, ]
dat.dropped = mydata.miss[Ry == 0, ]
dat.dropped
dat.cc
df_dich
df_imp <- cbind(df_imp, df_dich)
colnames(df_dich) <-
summary(df_dich)
colnames(df_dich) <-
summary(df_dich)
df_dich
# find the factor variables
idx_fac <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "factor") ))
df_fac <- df_20p_NA[,idx_fac]
# find the dichotomous variables
idx_dich <- which(unlist(lapply(df_fac, function(x)  length(levels(x)) )) == 2)
df_dich <- df_fac[,idx_dich]
colnames(df_dich) <-
summary(df_dich)
df_dich
colnames(df_dich) <-
str(df_dich)
colnames(df_dich)
df_dich
colnames(df_20p_NA)
colnames(df_fac)
colnames(df_dich)
# find the dichotomous variables
idx_dich <- which(unlist(lapply(df_fac, function(x)  length(levels(x)) )) == 2)
colnames(df_fac)
df_fac[ ,idx_dich]
df_fac[ ,idx_dich]
df_dich <- df_fac[ ,idx_dich]
colnames(df_dich)
colnames(df_dich) <- c("S1182", "S1626", "S1782")
str(df_dich)
summary(df_dich)
colnames(df_imp)
df_imp <- cbind(df_imp, df_dich)
which(is.na(df_imp$S1182))
mylogit <- glm(S1182 ~ M0298 + M0312 + M0360, data = dat.cc_S1182, family = "binomial")
dat.cc_S1182 = df_imp[which(!is.na(df_imp$S1182)), ]
dat.dropped_S1182 = df_imp[which(is.na(df_imp$S1182)), ]
mylogit <- glm(S1182 ~ M0298 + M0312 + M0360, data = dat.cc_S1182, family = "binomial")
summary(mylogit)
# We now use the model to predict the missing
y.imp <- predict(mylogit, newdata = dat.dropped_S1182, type = "response")
which(is.na(df_imp$S1182))
df_imp$S1182[which(is.na(df_imp$S1182))]
?rbinom
y.imp
rbinom(sum(is.na(df_imp$S1182)), 1, y.imp)
df_imp$S1182
rbinom(sum(is.na(df_imp$S1182)), 1, y.imp)
df_imp$S1182[1]
# with noise
df_imp$S1182[which(is.na(df_imp$S1182))] = rbinom(sum(is.na(df_imp$S1182)), 1, y.imp) == 1
ifelse(rbinom(sum(is.na(df_imp$S1182)), 1, y.imp) == 1,"Yes","No")
# with noise
df_imp$S1182[which(is.na(df_imp$S1182))] = ifelse(rbinom(sum(is.na(df_imp$S1182)), 1, y.imp) == 1,"Yes","No")
str(df_dich)
colnames(df_imp)
df_imp$S1626
df_imp$S1182
# read the data with around 20% missingness in the entire data set
df_20p_NA <- read.csv("~/Documents/NYU/Courses/Spring2021/MissingData/week3/data_20p_NA.csv")
# find the numerical variables
idx_num <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "numeric" |
lapply(df_20p_NA,class) == "integer")))
df_num <- df_20p_NA[idx_num]
# find which numerical variables have missing data
apply(is.na(df_num), 2, sum)
# find which variables are fully observed variables to be used in the regression imputation
as.numeric(apply(is.na(df_20p_NA), 2, sum))
# create a new data set to store the numerical variable to be imputed and the fully observed variables
df_imp <- with(df_20p_NA, data.frame( S1591 = S.1591.After.the.farmers.sell.some.of.the.crops.harvested.how.long.does.the.remaining.food.last...Months..max..12.,
M0298 = M.0298..Region, M0312 = M.0312..Returning.IDP..Individuals.,
M0360 = M.0360..Host.Community..Individual.))
str(df_imp)
# regression imputation with noise
data.cc = df_imp[which(!is.na(df_imp$S1591)), ]
data.dropped = df_imp[which(is.na(df_imp$S1591)), ]
reg = lm(S1591 ~ M0312 + M0360, data = data.cc)
y.imp = predict(reg, newdata = data.dropped)
noise = rnorm(length(y.imp), 0, summary(reg)$sigma)
df_imp$S1591[which(is.na(df_imp$S1591))] = y.imp + noise
# find the factor variables
idx_fac <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "factor") ))
df_fac <- df_20p_NA[,idx_fac]
# find the dichotomous variables
idx_dich <- which(unlist(lapply(df_fac, function(x)  length(levels(x)) )) == 2)
df_dich <- df_fac[ ,idx_dich]
colnames(df_dich) <- c("S1182", "S1626", "S1782")
str(df_dich)
df_imp <- cbind(df_imp, df_dich)
dat.cc_S1182 = df_imp[which(!is.na(df_imp$S1182)), ]
dat.dropped_S1182 = df_imp[which(is.na(df_imp$S1182)), ]
mylogit <- glm(S1182 ~ M0298 + M0312 + M0360, data = dat.cc_S1182, family = "binomial")
summary(mylogit)
# We now use the model to predict the missing
y.imp <- predict(mylogit, newdata = dat.dropped_S1182, type = "response")
rbinom(sum(is.na(df_imp$S1182)), 1, y.imp)
dat.cc_S1182$S1182
as.numeric(dat.cc_S1182$S1182)
df_imp$S1182
y.imp
rbinom(sum(is.na(df_imp$S1182)), 1, y.imp
)
mylogit
summary(mylogit)
df_imp$S1182
as.numeric(df_imp$S1182)
df_imp$S1626
colnames(df_imp)
df_imp$S1782
colnames(df_imp)
knitr::opts_chunk$set(echo = TRUE)
library(mice)
library(lmerTest)
# read the data with around 20% missingness in the entire data set
df_20p_NA <- read.csv("~/Documents/NYU/Courses/Spring2021/MissingData/week3/data_20p_NA.csv")
# find the numerical variables
idx_num <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "numeric" |
lapply(df_20p_NA,class) == "integer")))
df_num <- df_20p_NA[idx_num]
# find which numerical variables have missing data
apply(is.na(df_num), 2, sum)
# find which variables are fully observed variables to be used in the regression imputation
as.numeric(apply(is.na(df_20p_NA), 2, sum))
# create a new data set to store the numerical variable to be imputed and the fully observed variables
df_imp <- with(df_20p_NA, data.frame( S1591 = S.1591.After.the.farmers.sell.some.of.the.crops.harvested.how.long.does.the.remaining.food.last...Months..max..12.,
M0298 = M.0298..Region, M0312 = M.0312..Returning.IDP..Individuals.,
M0360 = M.0360..Host.Community..Individual.))
str(df_imp)
# regression imputation with noise
data.cc = df_imp[which(!is.na(df_imp$S1591)), ]
data.dropped = df_imp[which(is.na(df_imp$S1591)), ]
reg = lm(S1591 ~ M0312 + M0360, data = data.cc)
y.imp = predict(reg, newdata = data.dropped)
noise = rnorm(length(y.imp), 0, summary(reg)$sigma)
df_imp$S1591[which(is.na(df_imp$S1591))] = y.imp + noise
# find the factor variables
idx_fac <- as.numeric(which(unlist(lapply(df_20p_NA,class) == "factor") ))
df_fac <- df_20p_NA[,idx_fac]
# find the dichotomous variables
idx_dich <- which(unlist(lapply(df_fac, function(x)  length(levels(x)) )) == 2)
df_dich <- df_fac[ ,idx_dich]
colnames(df_dich) <- c("S1182", "S1626", "S1782")
str(df_dich)
df_imp <- cbind(df_imp, df_dich)
#### impute S1182
dat.cc_S1182 = df_imp[which(!is.na(df_imp$S1182)), ]
dat.dropped_S1182 = df_imp[which(is.na(df_imp$S1182)), ]
mylogit <- glm(S1182 ~ M0298 + M0312 + M0360, data = dat.cc_S1182, family = "binomial")
summary(mylogit)
# We now use the model to predict the missing
y.imp <- predict(mylogit, newdata = dat.dropped_S1182, type = "response")
# with noise
df_imp$S1182[which(is.na(df_imp$S1182))] = ifelse(rbinom(sum(is.na(df_imp$S1182)), 1, y.imp) == 1,"Yes","No")
#### impute S1626
dat.cc_S1626 = df_imp[which(!is.na(df_imp$S1626)), ]
dat.dropped_S1626 = df_imp[which(is.na(df_imp$S1626)), ]
mylogit <- glm(S1626 ~ M0298 + M0312 + M0360, data = dat.cc_S1626, family = "binomial")
summary(mylogit)
# We now use the model to predict the missing
y.imp <- predict(mylogit, newdata = dat.dropped_S1626, type = "response")
# with noise
df_imp$S1626[which(is.na(df_imp$S1626))] = ifelse(rbinom(sum(is.na(df_imp$S1626)), 1, y.imp) == 1,"Yes","No")
#### impute S1782
dat.cc_S1782 = df_imp[which(!is.na(df_imp$S1782)), ]
dat.dropped_S1782 = df_imp[which(is.na(df_imp$S1782)), ]
mylogit <- glm(S1782 ~ M0298 + M0312 + M0360, data = dat.cc_S1782, family = "binomial")
summary(mylogit)
# We now use the model to predict the missing
y.imp <- predict(mylogit, newdata = dat.dropped_S1782, type = "response")
# with noise
df_imp$S1782[which(is.na(df_imp$S1782))] = ifelse(rbinom(sum(is.na(df_imp$S1782)), 1, y.imp) == 1,"Yes","No")
# replace the numerical variable in the original data set with its imputed result
df_20p_NA$S.1591.After.the.farmers.sell.some.of.the.crops.harvested.how.long.does.the.remaining.food.last...Months..max..12. <- df_imp$S1591
df_20p_NA$S.1182.Is.there.a.health.facility.in.the.village. <- df_imp$S1182
df_20p_NA$S.1626.Do.the.majority.of.returning.IDPs.who.had.a.claim.to.land.in.this.village.currently.have.access.to.their.land. <- df_imp$S1626
df_20p_NA$S.1782.Is.there.a.mechanism.in.the.village.for.residents.to.make.a.complaint.or.give.feedback.about.services..assistance..problems..etc.. <- df_imp$S1782
# perform listwise deletion on nominal and ordinal variables would result in only 8 observations left
dim(na.omit(df_20p_NA))
# instead, apply complete cases only on the variables selected into the best model in the HW2
df_20p_NA_complete <- df_20p_NA
# create a correspondence table between the original and new variable names
correspondence <- data.frame(original = colnames(df_20p_NA_complete))
# create new variable names for the data set because the variable names are too long
for (i in 1:ncol(df_20p_NA_complete)) {
colnames(df_20p_NA_complete)[i] <-  paste("V",as.character(i), sep = "")
}
correspondence <- cbind(correspondence, colnames(df_20p_NA_complete))
# select the variables in the best model in the HW2
df_20p_NA_complete <- df_20p_NA_complete[,c("V1", "V2","V4", "V6", "V7", "V10",
"V11", "V13" ,"V14" , "V16" , "V17")]
# apply complete cases method
df_20p_NA_complete <- df_20p_NA_complete[complete.cases(df_20p_NA_complete),]
# run the best model selected by stepwise algorithm in the HW2
mod1 <- lmer(V2 ~ V4 + V6 + V7 + V10 + V11 + V13 + V14 + V16 + V17 + (1 | V1),
data = df_20p_NA_complete)
anova(mod1)
# read the original data set
df_original <- read.csv("~/Documents/NYU/Courses/Spring2021/MissingData/week3/original_data.csv")
# complete cases method on the entire dataset would result in only 23 observations left
dim(df_original[complete.cases(df_original),])
# instead, apply complete cases only on the variables selected into the best model in the HW2
df_original_complete <- df_original
# change the column names for the new data set df_original_complete same as df_20p_NA_complete
for (i in 1:ncol(df_original_complete)) {
colnames(df_original_complete)[i] <-  paste("V",as.character(i), sep = "")
}
df_original_complete <- df_original_complete[,c("V1", "V2","V4", "V6", "V7", "V10",
"V11", "V13" ,"V14" , "V16" , "V17")]
# apply complete cases method
df_original_complete <- df_original_complete[complete.cases(df_original_complete),]
# run the best model selected by stepwise algorithm in the HW2
mod2 <- lmer(V2 ~ V4 + V6 + V7 + V10 + V11 + V13 + V14 + V16 + V17 + (1 | V1),
data = df_original_complete)
anova(mod2)
