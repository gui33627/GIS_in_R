logit_V9 <- glm(V9 ~ V1 + V2 + V3, data = df_20p_NA, family = "binomial")
summary(logit_V9)
# use the model to predict the missing
V9.imp <- predict(logit_V9, newdata = ic(df_20p_NA[,c("V1","V2","V3","V9")]), type = "response")
# with noise
df_reg_noise$V9[which(is.na(df_20p_NA$V9))] <- ifelse(rbinom(sum(is.na(df_20p_NA$V9)), 1, V9.imp) == 1,"Yes","No")
mod_reg_noise <- lm(V2~., data = df_reg_noise)
summary(mod_reg_noise)
par(ask=F)
# Create the missing data frame object
mdf <- missing_data.frame(df_20p_NA)
# Five-number summary statistics + missing number
summary(mdf)
# Histograms of all variables with missing values
hist(mdf)
# Image of the data
image(mdf)
# Examine the default settings
show(mdf)
# Running the chains
imputations <- mi(mdf, seed = 100, n.iter = 20, n.chains = 2, parallel = F)
imputations <- mi(mdf, seed = 100, n.iter = 20, n.chains = 2, parallel = F)
# R-hat
Rhats(imputations)
imputations <- mi(mdf, seed = 100, n.iter = 20, n.chains = 2, parallel = F)
Rhats(imputations)
?mi
plot(imputations)[1]
plots <- plot(imputations)
plot(imputations, which=c(1,1))
?plot
plots <- plot(imputations, which=1)
par(ask = F)
plot(imputations, which=2)
par(bg = "white") # erase.screen() will appear not to work
# if the background color is transparent
# (as it is by default on most devices).
split.screen(c(2,1)) # split display into two screens
screen(1) # prepare screen 1 for output
plot(10:1)
screen(4) # prepare screen 4 for output
plot(10:1)
par(ask = F)
plot(imputations, which=c(1,1))
plot(imputations, which=c(1,1))
plot(imputations, which=1)
imputations
?plot
par(ask = F)
a <- plot(imputations, plot = F)
a
imputations
a <- plot(imputations)
a
a
a <- plot(imputations)
a
imputations[[4]]
imputations[4
]
imputations$V4
par(ask = F)
plot(imputations$V4)
par(ask = F)
plot(imputations[4])
subset(imputations,index_between=c(1,3,4) )
subset_nested.datlist(imputations,index_between=c(1,3,4) )
install.packages("miceadds")
miceadds::subset_datlist( imputations, select=c("V4", "V5")  )
miceadds::subset_datlist( datlist1a, index=c(4,5) )
miceadds::subset_datlist( imputations, index=c(4,5) )
miceadds::subset_datlist( imputations, index=c(4,5), toclass="imputationList" )
subset(imputations, select = "V4")
subset(imputations, select = V4)
subset(imputations, select = V4:V6)
plot(imputations)
knitr::opts_chunk$set(echo = TRUE)
library(mi)
library(mice)
library(nnet)
library(tidyverse)
# read the data with around 20% missingness in the entire data set
df_20p_NA <- read.csv("~/Documents/NYU/Courses/Spring2021/MissingData/week5/df_20p_NA_new.csv")
# percent of missing data in the data set
sum(apply( is.na(df_20p_NA), 2, sum))/(nrow(df_20p_NA)*ncol(df_20p_NA))
# create a correspondence table between the original and new variable names
correspondence <- data.frame(original = colnames(df_20p_NA))
# create new variable names for the data set because the variable names are too long
for (i in 1:ncol(df_20p_NA)) {
colnames(df_20p_NA)[i] <-  paste("V",as.character(i), sep = "")
}
correspondence <- cbind(correspondence, colnames(df_20p_NA))
summary(df_20p_NA)
df_listwise <- df_20p_NA[complete.cases(df_20p_NA),]
mod_listwise <- lm(V2~., data = df_listwise)
summary(mod_listwise)
# functions to compute mode and do mode imputation
mode = function(x)
{
ta = table(x)
tam = max(ta)
if (all(ta == tam))
mod = NA else
mod = names(ta)[ta == tam]
return(mod)
}
mode.imp <- function (a)
{
missing <- is.na(a)
a.obs <- a[!missing]
imputed <- a
imputed[missing] <- mode(a.obs)
# Output the imputed vector
return (imputed)
}
# columns of numerical class
df_num <- df_20p_NA[ ,2:4]
# mean imputation for the numeric variable
df_mean <- mice::mice(data.frame(df_num), method = "mean", m = 1, maxit = 1)
# create a new data set to save imputed results
df_mean_mode <- df_20p_NA
df_mean_mode[,2:4] <- mice::complete(df_mean)
# columns of categorical class
df_cat <- df_20p_NA[ ,5:10]
# save the mode imputation results to the new data set
for (i in 1:6) {
df_mean_mode[,i+4] <- mode.imp(df_cat[,i])
}
summary(df_mean_mode)
mod_mean_mode <- lm(V2~., data = df_mean_mode)
summary(mod_mean_mode)
random.imp <- function (a)
{
missing <- is.na(a)
n.missing <- sum(missing)
a.obs = a[!missing] # Observed data
imputed <- a
imputed[missing] <- sample (a.obs, n.missing, replace=TRUE)
return (imputed)
}
# create a new data set to save imputed results
df_random <- df_20p_NA
# save the mode imputation results to the new data set
for (i in 4:10) {
df_random[,i] <- random.imp(df_20p_NA[,i])
}
summary(df_random)
mod_random <- lm(V2~., data = df_random)
summary(mod_random)
df_dummy <- df_mean_mode
# add columns indicating if missing or not for each non-complete variables
df_dummy <- is.na(df_20p_NA[, 4:10]) %>% as_tibble() %>%
setNames(paste0(names(.), '.NA')) %>% bind_cols(df_dummy, .)
mod_dummy <- lm(V2~., data = df_dummy)
summary(mod_dummy)
df_hotdeck <- df_20p_NA
df_hotdeck <- VIM::hotdeck(df_hotdeck, variable = c("V4","V5", "V6", "V7", "V8", "V9","V10"))
mod_hotdeck <- lm(V2~., data = df_hotdeck)
summary(mod_hotdeck)
df_reg <- df_20p_NA
df_reg[,1:4] <- complete(mice(data.frame(df_20p_NA[,1:4]), method = "norm.predict", m = 1, maxit = 1))
# V6
logit_V6 <- glm(V6 ~ V1 + V2 + V3, data = df_20p_NA, family = "binomial")
summary(logit_V6)
# use the model to predict the missing
V6.imp <- predict(logit_V6, newdata = ic(df_20p_NA[,c("V1","V2","V3","V6")]), type = "response")
# V9
logit_V9 <- glm(V9 ~ V1 + V2 + V3, data = df_20p_NA, family = "binomial")
summary(logit_V9)
# use the model to predict the missing
V9.imp <- predict(logit_V9, newdata = ic(df_20p_NA[,c("V1","V2","V3","V9")]), type = "response")
df_reg$V6[which(is.na(df_reg$V6))] <- ifelse(V6.imp > 0.5,"Yes","No")
df_reg$V9[which(is.na(df_reg$V9))] <- ifelse(V9.imp > 0.5,"Yes","No")
# fit model for V5 on observed data
multi_V5 <- multinom(V5 ~ V1 + V2 + V3, data = df_20p_NA)
# predict missing data in V5
df_reg$V5[is.na(df_reg$V5)] <- predict(multi_V5, newdata = ic(df_reg[, c("V1", "V2","V3","V5")]),
type = "class")
# fit model for V7 on observed data
multi_V7 <- multinom(V7 ~ V1 + V2 + V3, data = df_20p_NA)
# predict missing data in V7
df_reg$V7[is.na(df_reg$V7)] <- predict(multi_V7, newdata = ic(df_reg[, c("V1", "V2","V3","V7")]),
type = "class")
# fit model for V8 on observed data
multi_V8 <- multinom(V8 ~ V1 + V2 + V3, data = df_20p_NA)
# predict missing data in V8
df_reg$V8[is.na(df_reg$V8)] <- predict(multi_V8, newdata = ic(df_reg[, c("V1", "V2","V3","V8")]),
type = "class")
# fit model for V10 on observed data
multi_V10 <- multinom(V10 ~ V1 + V2 + V3, data = df_20p_NA)
# predict missing data in V10
df_reg$V10[is.na(df_reg$V10)] <- predict(multi_V10, newdata = ic(df_reg[, c("V1", "V2","V3","V10")]),
type = "class")
mod_reg <- lm(V2~., data = df_reg)
summary(mod_reg)
# a new data set to store all regression imputation with noise results
df_reg_noise <- df_20p_NA
# a data set for regression imputation with noise for numerical variable
df_imp <- df_20p_NA[,1:4]
# regression imputation with noise
data.cc <- df_imp[which(!is.na(df_imp$V4)), ]
data.dropped <- df_imp[which(is.na(df_imp$V4)), ]
reg <- lm(V4 ~ V2 + V3, data = data.cc)
y.imp <- predict(reg, newdata = data.dropped)
noise <- rnorm(length(y.imp), 0, summary(reg)$sigma)
df_reg_noise$V4[which(is.na(df_reg_noise$V4))] <- y.imp + noise
# V6
logit_V6 <- glm(V6 ~ V1 + V2 + V3, data = df_20p_NA, family = "binomial")
summary(logit_V6)
# use the model to predict the missing
V6.imp <- predict(logit_V6, newdata = ic(df_20p_NA[,c("V1","V2","V3","V6")]), type = "response")
# with noise
df_reg_noise$V6[which(is.na(df_20p_NA$V6))] <- ifelse(rbinom(sum(is.na(df_20p_NA$V6)), 1, V6.imp) == 1,"Yes","No")
# V9
logit_V9 <- glm(V9 ~ V1 + V2 + V3, data = df_20p_NA, family = "binomial")
summary(logit_V9)
# use the model to predict the missing
V9.imp <- predict(logit_V9, newdata = ic(df_20p_NA[,c("V1","V2","V3","V9")]), type = "response")
# with noise
df_reg_noise$V9[which(is.na(df_20p_NA$V9))] <- ifelse(rbinom(sum(is.na(df_20p_NA$V9)), 1, V9.imp) == 1,"Yes","No")
mod_reg_noise <- lm(V2~., data = df_reg_noise)
summary(mod_reg_noise)
par(ask=F)
# Create the missing data frame object
mdf <- missing_data.frame(df_20p_NA)
# Five-number summary statistics + missing number
summary(mdf)
# Histograms of all variables with missing values
hist(mdf)
# Image of the data
image(mdf)
# Examine the default settings
show(mdf)
# Running the chains
imputations <- mi(mdf, seed = 100, parallel = F)
par(ask = F)
# The result is an array of matrices with all imputed variables means and sd's
converged = mi2BUGS(imputations)
# Extract specific variables from the imputations
mean_V4 = converged[, , 1]
mean_V5 = converged[, , 2]
mean_V6 = converged[, , 3]
mean_V7 = converged[, , 4]
mean_V8 = converged[, , 5]
mean_V9 = converged[, , 6]
mean_V10 = converged[, , 7]
par(mfrow = c(2,4))
# Traceplot of mean imputed values of V4
ts.plot(mean_V4[,1], col=1, ylim = c(-0.07, 0.02))
lines(mean_V4[,2], col= 2)
lines(mean_V4[,3], col= 3)
lines(mean_V4[,4], col= 4)
# Traceplot of mean imputed values of V5
ts.plot(mean_V5[,1], col=1, ylim = c(1.18, 1.23))
lines(mean_V5[,2], col= 2)
lines(mean_V5[,3], col= 3)
lines(mean_V5[,4], col= 4)
# Traceplot of mean imputed values of V6
ts.plot(mean_V6[,1], col=1, ylim = c(1.21, 1.29))
lines(mean_V6[,2], col= 2)
lines(mean_V6[,3], col= 3)
lines(mean_V6[,4], col= 4)
# Traceplot of mean imputed values of V7
ts.plot(mean_V7[,1], col=1, ylim = c(6.4, 6.76))
lines(mean_V7[,2], col= 2)
lines(mean_V7[,3], col= 3)
lines(mean_V7[,4], col= 4)
# Traceplot of mean imputed values of V8
ts.plot(mean_V8[,1], col=1, ylim = c(1.9, 1.98))
lines(mean_V8[,2], col= 2)
lines(mean_V8[,3], col= 3)
lines(mean_V8[,4], col= 4)
# Traceplot of mean imputed values of V9
ts.plot(mean_V9[,1], col=1, ylim = c(1.65, 1.73))
lines(mean_V9[,2], col= 2)
lines(mean_V9[,3], col= 3)
lines(mean_V9[,4], col= 4)
# Traceplot of mean imputed values of V10
ts.plot(mean_V10[,1], col=1, ylim = c(1.06, 1.12))
lines(mean_V10[,2], col= 2)
lines(mean_V10[,3], col= 3)
lines(mean_V10[,4], col= 4)
# R-hat
Rhats(imputations)
par(ask = F)
# The result is an array of matrices with all imputed variables means and sd's
converged = mi2BUGS(imputations)
# Extract specific variables from the imputations
mean_V4 = converged[, , 1]
mean_V5 = converged[, , 2]
mean_V6 = converged[, , 3]
mean_V7 = converged[, , 4]
mean_V8 = converged[, , 5]
mean_V9 = converged[, , 6]
mean_V10 = converged[, , 7]
par(mfrow = c(2,4))
# Traceplot of mean imputed values of V4
ts.plot(mean_V4[,1], col=1, ylim = c(-0.07, 0.02))
lines(mean_V4[,2], col= 2)
lines(mean_V4[,3], col= 3)
lines(mean_V4[,4], col= 4)
# Traceplot of mean imputed values of V5
ts.plot(mean_V5[,1], col=1, ylim = c(1.18, 1.23))
lines(mean_V5[,2], col= 2)
lines(mean_V5[,3], col= 3)
lines(mean_V5[,4], col= 4)
# Traceplot of mean imputed values of V6
ts.plot(mean_V6[,1], col=1, ylim = c(1.21, 1.29))
lines(mean_V6[,2], col= 2)
lines(mean_V6[,3], col= 3)
lines(mean_V6[,4], col= 4)
# Traceplot of mean imputed values of V7
ts.plot(mean_V7[,1], col=1, ylim = c(6.4, 6.76))
lines(mean_V7[,2], col= 2)
lines(mean_V7[,3], col= 3)
lines(mean_V7[,4], col= 4)
# Traceplot of mean imputed values of V8
ts.plot(mean_V8[,1], col=1, ylim = c(1.9, 1.98))
lines(mean_V8[,2], col= 2)
lines(mean_V8[,3], col= 3)
lines(mean_V8[,4], col= 4)
# Traceplot of mean imputed values of V9
ts.plot(mean_V9[,1], col=1, ylim = c(1.65, 1.73))
lines(mean_V9[,2], col= 2)
lines(mean_V9[,3], col= 3)
lines(mean_V9[,4], col= 4)
# Traceplot of mean imputed values of V10
ts.plot(mean_V10[,1], col=1, ylim = c(1.06, 1.12))
lines(mean_V10[,2], col= 2)
lines(mean_V10[,3], col= 3)
lines(mean_V10[,4], col= 4)
# R-hat
Rhats(imputations)
# Running the chains
imputations1 <- mi(mdf, seed = 100, n.iter = 60, n.chains = 5, parallel = F)
# R-hat
Rhats(imputations1)
plot(imputations)
ts.plot(mean_V10[,1], col=1, ylim = c(1.06, 1.12))
lines(mean_V10[,3], col= 3)
lines(mean_V10[,4], col= 4)
par(ask = F)
# The result is an array of matrices with all imputed variables means and sd's
converged = mi2BUGS(imputations)
# Extract specific variables from the imputations
mean_V4 = converged[, , 1]
mean_V5 = converged[, , 2]
mean_V6 = converged[, , 3]
mean_V7 = converged[, , 4]
mean_V8 = converged[, , 5]
mean_V9 = converged[, , 6]
mean_V10 = converged[, , 7]
par(mfrow = c(2,4))
# Traceplot of mean imputed values of V4
ts.plot(mean_V4[,1], col=1, ylim = c(-0.07, 0.02))
lines(mean_V4[,2], col= 2)
lines(mean_V4[,3], col= 3)
lines(mean_V4[,4], col= 4)
# Traceplot of mean imputed values of V5
ts.plot(mean_V5[,1], col=1, ylim = c(1.18, 1.23))
lines(mean_V5[,2], col= 2)
lines(mean_V5[,3], col= 3)
lines(mean_V5[,4], col= 4)
# Traceplot of mean imputed values of V6
ts.plot(mean_V6[,1], col=1, ylim = c(1.21, 1.29))
lines(mean_V6[,2], col= 2)
lines(mean_V6[,3], col= 3)
lines(mean_V6[,4], col= 4)
# Traceplot of mean imputed values of V7
ts.plot(mean_V7[,1], col=1, ylim = c(6.4, 6.76))
lines(mean_V7[,2], col= 2)
lines(mean_V7[,3], col= 3)
lines(mean_V7[,4], col= 4)
# Traceplot of mean imputed values of V8
ts.plot(mean_V8[,1], col=1, ylim = c(1.9, 1.98))
lines(mean_V8[,2], col= 2)
lines(mean_V8[,3], col= 3)
lines(mean_V8[,4], col= 4)
# Traceplot of mean imputed values of V9
ts.plot(mean_V9[,1], col=1, ylim = c(1.65, 1.73))
lines(mean_V9[,2], col= 2)
lines(mean_V9[,3], col= 3)
lines(mean_V9[,4], col= 4)
# Traceplot of mean imputed values of V10
ts.plot(mean_V10[,1], col=1, ylim = c(1.06, 1.12))
lines(mean_V10[,2], col= 2)
lines(mean_V10[,3], col= 3)
lines(mean_V10[,4], col= 4)
par(mfrow = c(2,4))
# Traceplot of mean imputed values of V4
ts.plot(mean_V4[,1], col=1, ylim = c(-0.07, 0.02))
lines(mean_V4[,3], col= 3)
lines(mean_V4[,4], col= 4)
# Traceplot of mean imputed values of V5
ts.plot(mean_V5[,1], col=1, ylim = c(1.18, 1.23))
lines(mean_V5[,2], col= 2)
lines(mean_V5[,3], col= 3)
lines(mean_V5[,4], col= 4)
# Traceplot of mean imputed values of V6
ts.plot(mean_V6[,1], col=1, ylim = c(1.21, 1.29))
lines(mean_V6[,2], col= 2)
lines(mean_V6[,3], col= 3)
lines(mean_V6[,4], col= 4)
# Traceplot of mean imputed values of V7
ts.plot(mean_V7[,1], col=1, ylim = c(6.4, 6.76))
lines(mean_V7[,2], col= 2)
lines(mean_V7[,3], col= 3)
lines(mean_V7[,4], col= 4)
# Traceplot of mean imputed values of V8
ts.plot(mean_V8[,1], col=1, ylim = c(1.9, 1.98))
lines(mean_V8[,2], col= 2)
lines(mean_V8[,3], col= 3)
lines(mean_V8[,4], col= 4)
# Traceplot of mean imputed values of V9
ts.plot(mean_V9[,1], col=1, ylim = c(1.65, 1.73))
lines(mean_V9[,2], col= 2)
lines(mean_V9[,3], col= 3)
lines(mean_V9[,4], col= 4)
# Traceplot of mean imputed values of V10
ts.plot(mean_V10[,1], col=1, ylim = c(1.06, 1.12))
lines(mean_V10[,2], col= 2)
lines(mean_V10[,3], col= 3)
lines(mean_V10[,4], col= 4)
par(mfrow = c(2,4))
# Traceplot of mean imputed values of V4
ts.plot(mean_V4[,1], col=1, ylim = c(-0.07, 0.02))
lines(mean_V4[,3], col= 3)
lines(mean_V4[,4], col= 4)
# Traceplot of mean imputed values of V5
ts.plot(mean_V5[,1], col=1, ylim = c(1.17, 1.22))
lines(mean_V5[,2], col= 2)
lines(mean_V5[,3], col= 3)
lines(mean_V5[,4], col= 4)
# Traceplot of mean imputed values of V6
ts.plot(mean_V6[,1], col=1, ylim = c(1.2, 1.29))
lines(mean_V6[,2], col= 2)
lines(mean_V6[,3], col= 3)
lines(mean_V6[,4], col= 4)
# Traceplot of mean imputed values of V7
ts.plot(mean_V7[,1], col=1, ylim = c(6.4, 6.8))
lines(mean_V7[,2], col= 2)
lines(mean_V7[,3], col= 3)
lines(mean_V7[,4], col= 4)
# Traceplot of mean imputed values of V8
ts.plot(mean_V8[,1], col=1, ylim = c(1.9, 1.98))
lines(mean_V8[,2], col= 2)
lines(mean_V8[,3], col= 3)
lines(mean_V8[,4], col= 4)
# Traceplot of mean imputed values of V9
ts.plot(mean_V9[,1], col=1, ylim = c(1.65, 1.73))
lines(mean_V9[,2], col= 2)
lines(mean_V9[,3], col= 3)
lines(mean_V9[,4], col= 4)
# Traceplot of mean imputed values of V10
ts.plot(mean_V10[,1], col=1, ylim = c(1.07, 1.12))
lines(mean_V10[,2], col= 2)
lines(mean_V10[,3], col= 3)
lines(mean_V10[,4], col= 4)
mean_V4[,1]
mean_V4[,2]
ts.plot(mean_V4[,1], col=1, ylim = c(-0.07, 0.02))
lines(mean_V4[,2], col= 2)
lines(mean_V4[,3], col= 3)
lines(mean_V4[,4], col= 4)
plot(imputations)
plot(imputations1)
# Try pmm for the variables
mdf <-  missing_data.frame(df_20p_NA)
mdf <- change(mdf, y = c("V5", "V7", "V8", "V10"),
what ="imputation_method", to = "pmm")
show(mdf)
plot(imputations1, which = "V4")
plot(imputations1, which = "V4")
# Try pmm for the variables
mdf <-  missing_data.frame(df_20p_NA)
mdf <- change(mdf, y = c("V5", "V7", "V8", "V10"),
what ="imputation_method", to = "pmm")
show(mdf)
imputations2 <- mi(mdf, seed = 100, n.iter = 50, n.chains = 5, parallel = F)
# R-hat
Rhats(imputations2)
# Try pmm for the variables
mdf <-  missing_data.frame(df_20p_NA)
mdf <- change(mdf, y = c("V4", "V5", "V6","V7", "V8", "V10"),
what ="imputation_method", to = "pmm")
show(mdf)
# Try pmm for the variables
mdf <-  missing_data.frame(df_20p_NA)
mdf <- change(mdf, y = c("V4", "V5", "V6","V7", "V8", "V10"),
what ="imputation_method", to = "pmm")
show(mdf)
imputations2 <- mi(mdf, seed = 100, n.iter = 50, n.chains = 5, parallel = F)
Rhats(imputations2)
analysis <- pool(V2 ~ V1 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10, imputations2, m=5)
display(analysis)
analysis <- mi::pool(V2 ~ V1 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10, imputations2, m=5)
mi::display(analysis)
library(sandwich)
export_summs(mod_dummy, mod_mean_mode, analysisl)
?tbl_regression
??tbl_regression
devtools::install_github("ewenharrison/finalfit")
remove.packages("finalfit")
stargazer::stargazer(mod_dummy, mod_mean_mode, analysis)
library(startazer)
library(stargazer)
?stargazer
stargazer::stargazer(mod_dummy, mod_mean_mode, analysis, type = "text")
mi::display(analysis)
stargazer::stargazer(mod_dummy, mod_mean_mode, type = "text")
stargazer::stargazer(mod_listwise, mod_random, mod_dummy, mod_mean_mode, mod_hotdeck, mod_reg, mod_reg_noise, type = "text")
setwd("~/Documents/R/GIS/Howto/GIS_in_R.merging")
